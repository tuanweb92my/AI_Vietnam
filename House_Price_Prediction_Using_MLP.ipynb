{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptrons\n",
    "\n",
    "A perceptron produces a single output based on several real-valued inputs by forming a linear combination using its input weights and passing the output through a nonlinear activation function. A perceptron is computed as a linear combination of all the nodes from the previous layers as\n",
    "$y=\\delta \\left ( \\sum_{0}^{n} w_ix_i \\right ) = \\delta \\left ( w^Tx \\right )$ where `w` denotes the vector of weights, `x` is the vector of inputs, $x_0 = 1$, $w_0$ is the bias weight, and \\delta is the non-linear activation function.\n",
    "\n",
    "A multilayer perceptron (MLP) is a deep, artificial neural network. It is composed of more than one perceptron. They are composed of an input layer to receive the signal, an output layer that makes a decision or prediction about the input, and in between those two, an arbitrary number of hidden layers that are the true computational engine of the MLP. MLPs with one hidden layer are capable of approximating any continuous function.\n",
    "\n",
    "Multilayer perceptrons are often applied to supervised learning problems: they train on a set of input-output pairs and learn to model the correlation (or dependencies) between those inputs and outputs. Training involves adjusting the parameters, or the weights and biases, of the model in order to minimize error. Backpropagation is used to make those weigh and bias adjustments relative to the error, and the error itself can be measured in a variety of ways, including by root mean squared error (RMSE).\n",
    "\n",
    "In the forward pass, the signal flow moves from the input layer through the hidden layers to the output layer, and the decision of the output layer is measured against the ground truth labels.\n",
    "\n",
    "In the backward pass, using backpropagation and the chain rule of calculus, partial derivatives of the error function w.r.t. the various weights and biases are back-propagated through the MLP. That act of differentiation gives us a gradient, or a landscape of error, along which the parameters may be adjusted as they move the MLP one step closer to the error minimum. This can be done with any gradient-based optimisation algorithm such as stochastic gradient descent.\n",
    "\n",
    "#### Examples of MLPs\n",
    "\n",
    "1) A multi-layer perceptron that has a hidden layer.\n",
    " - Input layer has 2 nodes\n",
    " - Hidden layer has 2 nodes\n",
    " - Output layer has 1 node\n",
    " \n",
    "<img src=\"images/fig1.png\" width=\"500\">\n",
    "\n",
    "Mỗi layer có một node không nhận giá trị từ các node ở layer phía trước, mà luôn có giá trị là `1`. Khi phác họa MLP, đôi khi node này được ngầm hiểu sự tồn tại và không được vẽ ra. Tuy nhiên, để hiểu đúng và đầy đủ các thành phần và cách thức hoạt động của MLP, cấu trúc của network được trình bày như sau:\n",
    "\n",
    "<img src=\"images/fig11.png\" width=\"500\">\n",
    " \n",
    "2) A multi-layer perceptron that has two hidden layers.\n",
    " - Input layer has 2 nodes\n",
    " - Hidden layer 1 has 2 nodes\n",
    " - Hidden layer 2 has 3 nodes\n",
    " - Output layer has 1 node\n",
    "\n",
    "<img src=\"images/fig22.png\" width=\"600\">\n",
    "\n",
    "3) Giả sử chúng ta cần thiết kế một MLP để dự đoán giá nhà dựa vào hai tiêu chí: diện tích nhà ($m^2$) và khoảng cách đến trung tâm thành phố (km).\n",
    "\n",
    "Dựa vào yêu cầu bài toán, chúng ta có thể xác định input layer có 2 node và output layer có 1 node. Giả sử chúng ta muốn network có 1 hidden layer gồm 2 node, network cho bài toán này cho cấu trúc như sau:\n",
    "\n",
    "<img src=\"images/fig3.png\" width=\"600\">\n",
    "\n",
    "Hai giá trị $a_1$ và $a_2$ được tính như sau:\n",
    "\n",
    "$\n",
    "a_1 = \\delta \\left ( x_1  \\times w_1  + x_2  \\times w_3  + x_3  \\times w_5 \\right )\\\\\n",
    "a_2 = \\delta \\left ( x_1  \\times w_2  + x_2  \\times w_4  + x_3  \\times w_6 \\right )\n",
    "$\n",
    "\n",
    "Để thấy rõ hơn các bước tính toán cho giá trị $a_1$ và $a_2$, network có thể vẽ lại chi tiết hơn như sau:\n",
    "\n",
    "<img src=\"images/fig4.png\" width=\"600\">\n",
    "\n",
    "Lúc này,\n",
    "$\n",
    "z_1 = x_1  \\times w_1  + x_2  \\times w_3  + x_3  \\times w_5\\\\\n",
    "z_2 = x_1  \\times w_2  + x_2  \\times w_4  + x_3  \\times w_6\\\\\n",
    "a_1 = \\delta \\left ( z_1 \\right )\\\\\n",
    "a_2 = \\delta \\left ( z_2 \\right )\n",
    "$\n",
    "\n",
    "Hàm activation $\\delta \\left ( \\right )$ được dùng phổ biến hiện nay là hàm ReLU, được định nghĩa `y = max(0, x)`. Ý nghĩa của hàm ReLU là khi giá trị `x` lớn hơn `0` thì trả về chính giá trị đó; ngược lại trả về `0`. Hàm ReLU được phác họa như sau:\n",
    "\n",
    "<img src=\"images/fig5.png\" width=\"600\">\n",
    "\n",
    "Khi xác định dùng hàm ReLU, giá trị $a_1$ và $a_2$ được tính như sau:\n",
    "\n",
    "$\n",
    "a_1 = ReLU \\left ( z_1 \\right )\\\\\n",
    "a_2 = ReLU \\left ( z_2 \\right )\n",
    "$\n",
    "\n",
    "Network chúng ta thiết kế có 9 biến trọng số. Khi network được khởi tạo, 9 biến trọng số này được gán giá trị ngẫu nhiên. Quá trình train (huấn luyện) network là quá trình thay đổi các giá trị của các biến trọng số sao cho network hoạt động chính xác hơn.\n",
    "\n",
    "Để huấn luyện network theo phương pháp supervised learning, chúng ta cần bộ dữ liệu mẫu (training data) bao gồm (`diện-tích-nhà`, `khoảng-cách-đến-trung-tâm`, `giá-nhà`). Dữ liệu `diện-tích-nhà` và `khoảng-cách-đến-trung-tâm` được dùng để truyền cho $x_1$ và $x_2$. Dựa vào giá trị của các biến trọng số, chúng ta có thể tính được giá trị của $o_1$ (chính là giá nhà mà network tiên đoán). Độ chính xác của network được tính dựa vào sự khác biệt giữa $o_1$ và `giá-nhà`. \n",
    "\n",
    "Ví dụ, tiên giá nhà giá nhà với `diện-tích-nhà = 60`($m^2$), `khoảng-cách-đến-trung-tâm = 20 (km)`. Các giá trị biến trọng số được gán với các giá trị nguyên để cho đơn giản việc tính toán. Chú ý rằng trong thực tế, giá trị của các biến trọng số thường là số thực. Hình sau thể hiện network với các giá trị input, tham số, và output.\n",
    "\n",
    "<img src=\"images/fig6.png\" width=\"600\">\n",
    "\n",
    "$\n",
    " x_1  = 60 \\\\ \n",
    " x_2  = 20 \\\\ \n",
    " z_1  = x_1  \\times w_1  + x_2  \\times w_3  + 1  \\times w_5  = 60 \\times 1.0 + 20 \\times 1.0 + 1 \\times 2.0 = 82.0 \\\\ \n",
    " z_2  = x_1  \\times w_2  + x_2  \\times w_4  + 1  \\times w_6  = 60 \\times ( - 2.0) + 20 \\times ( - 3.0) + 1 \\times 1.0 =  - 179.0 \\\\ \n",
    " a_1  = {\\mathop{\\rm Re}\\nolimits} LU(z_1 ) = {\\mathop{\\rm Re}\\nolimits} LU(82.0) = 82.0 \\\\ \n",
    " a_2  = {\\mathop{\\rm Re}\\nolimits} LU(z_2 ) = {\\mathop{\\rm Re}\\nolimits} LU( - 179.0) = 0 \\\\ \n",
    " o_1  = a_1  \\times w_7  + a_2  \\times w_8  + 1  \\times w_9 = 85 \\\\ \n",
    "$\n",
    "\n",
    "#### Qui trình huấn luyện MLP\n",
    "\n",
    "Qui trình huấn luyện MLP theo phương pháp supervised learning gồm 5 bước, được mô tả trong hình sau: \n",
    "\n",
    "<img src=\"images/step.png\" width=\"300\">\n",
    "\n",
    "Từ bước 3 đến bước 5 được thực hiện nhiều lần cho đến khi nào điều kiện dừng lại được thỏa mãn. Các điền kiện dừng thường dùng là số lần huấn luyện hết một lượt training data (hay còn gọi là số epoch).\n",
    "\n",
    "1) Chuẩn bị training data\n",
    "\n",
    "Training data là bộ dữ liệu được thu thập từ thực tế. Ví dụ trong bài toán tiên đoán giá nhà ở trên, mỗi mẫu (sample) thu thập bao gồm diện tích nhà, khoảng cách đến trung tâm thành phố và giá nhà thực tế. Trong đó, diện tích nhà và khoảng cách đến trung tâm thành phố là dữ liệu đặc trưng (feature) và là giá trị đầu vào cho network, còn giá nhà thực tế là dữ liệu thực (label hay ground truth) và được dùng để tính loss bằng cách so sánh nó với giá trị output của network. Hình sau minh họa training data cho bài toán tiên đoán giá nhà.\n",
    "\n",
    "<img src=\"images/p1.png\" width=\"300\">\n",
    "\n",
    "2) Thiết kế network\n",
    "\n",
    "Ở bước này, dựa vào mô tả bài toán, chúng ta có thể xác định được số node cần thiết trong input layer và output layer. Ví dụ cho bài toán tiên đoán giá nhà, input layer bao gồm 2 node, tương ứng với 2 thuộc tính là diện tích nhà và khoảng cách đến trung tâm thành phố. Output layer bao gồm 1 node; chính là giá nhà tiên đoán.\n",
    "\n",
    "Việc tiếp theo là chọn lựa số lượng hidden layer và số node trong mỗi hidden layer. Đây là một vấn đề mở, nghĩa là không có một lý thuyết hay công thức nào xác định con số tối ưu cho số hidden layer và số node trong đó. Những con số này thường được lựa chọn dựa vào độ lớn của training data, số node trong input layer và output layer, và độ phức tạp của bài toán.\n",
    "\n",
    "Sau khi thiết kế network xong, chung ta có thể xác định được số lượng biến trọng số. Quay trở lại với ví dụ tiên đoán giá nhà, network với 1 hidden layer, bao gồm 2 node, có 9 biến trọng số, được minh họa trong hình sau:\n",
    "\n",
    "<img src=\"images/fig3.png\" width=\"600\">\n",
    "\n",
    "3) Forward pass (tính output)\n",
    "\n",
    "Ở bước này, dữ liệu đặc trưng được đưa vào network để tính giá trị output, cũng chính là giá nhà tiên đoán. Cách thức tính output đã được trình bày ở phần trên. Người đọc xem lại nếu nắm chưa vững phần này.\n",
    "\n",
    "\n",
    "<img src=\"images/p2.png\" width=\"500\">\n",
    "\n",
    "4) Tính loss\n",
    "\n",
    "Dựa vào giá nhà tiên đoán và giá nhà thực, chúng ta có thể tính được mức độ sai (không chính xác) của network với các biến trọng số hiện tại. Hàm loss được dùng phổ biến trong các bài toán tiên đoán (prediction) là hàm mean-squared-error (MSE). MSE của 2 vector `a` và `b` được tính như sau:\n",
    "\n",
    "$\n",
    "MSE = \\frac{1}{n} \\sum_{1}^{n} (a - b)^2\n",
    "$\n",
    "\n",
    "Hình sau minh họa việc tính loss dựa vào giá nhà tiên đoán và giá nhà thưc.\n",
    "\n",
    "<img src=\"images/p3.png\" width=\"500\">\n",
    "\n",
    "5) Cập nhập giá trị biến trọng số\n",
    "\n",
    "Ở bước này, thuật toán back-propagation được sử dụng để tính giá trị đạo hàm cho từng biến trọng số. Biết được giá trị đạo hàm cho một biến đồng nghĩa với việc xác định được hướng của biến (tăng hay giảm giá trị cho biến đó) để network hoạt động tốt hơn (loss giảm).\n",
    "\n",
    "Lúc này, các biến trọng số được thay đổi giá trị theo giá trị đạo hàm đã tính trước đó. Tuy nhiên, cách thức thay đổi giá trị các biến trọng số này có nhiều cách, đồng nghĩa với việc có rất nhiều thuật toán thực hiện điều này như stochastic gradient descent hay Adam. Thực ra, sự khác biệt của các thuật toán này không nhiều khi số epoch đủ lớn. Nếu không có lý do gì đặc biệt, thuật toán Adam thường được dùng rộng rãi.\n",
    "\n",
    "Hình sau mô tả việc tính giá trị đạo hàm và thay đổi giá trị cho các biến trọng số (hình này cần vẽ lại chi tiết hơn): \n",
    "\n",
    "<img src=\"images/p4.png\" width=\"500\">\n",
    "\n",
    "#### Thảo luận\n",
    "\n",
    "Qui trình huấn luyện network ở trên có đề cập đến một số thuật toán như back-propagation và Adam, nhưng những thuật toán này không được trình này chi tiết. Có vài lý do cho việc này:\n",
    " - Chưa cần thiết cho người học ở dạng beginner và intermediate. Các thư viện deep learning hiện nay đã trang bị hầu hết các thuật toán phổ biến, và chúng ta có thể coi các thuật toán này như các công cụ thể hoàn thành bài toán đang xem xét.\n",
    " - Hiểu chi tiết các thuật toán này hữu ích khi bạn muốn phát triển một thuật toán thay thế, cho kết quả chính xác hơn hay nhanh hơn. Cái này thuộc loại khó. Các bạn nên dành thời gian và công sức học những cái giúp bản thân làm được nhiều thứ hơn.\n",
    " - Việc không nắm được chi tiết các thuật toán này hầu như không ảnh hưởng tới việc giải quyết các bài toán trong thực tế và trong nghiên cứu.\n",
    " - Sức người có hạn, nhiều cái chúng ta nên sử dụng dạng black-box.\n",
    " \n",
    "Ở mức độ hiện tại, khi thiết kế và huấn luyện một network, chúng ta có thể sử dụng \n",
    " - ReLU cho hàm activation, \n",
    " - Dùng MSE để tính loss cho bài toán prediction\n",
    " - Dùng Adam để cập nhật biến trọng số\n",
    "\n",
    "\n",
    "# Ứng dụng cho bài toán tiên đoán giá nhà dùng Boston data\n",
    "\n",
    "\n",
    "#### Data description\n",
    "\n",
    "Boston dataset has 13 properties and a label (medv). These properties are presented in the following table.\n",
    "\n",
    "| Plugin     | <p align=\"center\">README </p>|\n",
    "| ---------- | ------------- |\n",
    "| **crim**   | <p align=\"center\"> per capita crime rate by town </p> |\n",
    "| **zn**     | <p align=\"center\">proportion of residential land zoned for lots over 25,000 sq.ft</p> |\n",
    "|  **indus** | <p align=\"center\">proportion of non-retail business acres per town </p> |\n",
    "| **chas**   | <p align=\"center\">Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) </p> |\n",
    "|  **nox**   | <p align=\"center\">nitrogen oxides concentration (parts per 10 million)</p>  |\n",
    "|  rm        | <p align=\"center\">average number of rooms per dwelling </p> |\n",
    "|  **age**   | <p align=\"center\">proportion of owner-occupied units built prior to 1940 </p> |\n",
    "| **dis**    | <p align=\"center\">weighted mean of distances to five Boston employment centres </p> |\n",
    "|  **rad**   | <p align=\"center\">index of accessibility to radial highways </p> |\n",
    "|  **tax**   | <p align=\"center\">full-value property-tax rate per $\\$$10,000</p> |\n",
    "|  ptratio   | <p align=\"center\">pupil-teacher ratio by town</p> |\n",
    "|  **black** | <p align=\"center\">1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</p> |\n",
    "|  lstat     | <p align=\"center\">lower status of the population (percent)</p> |\n",
    "|  **medv**  | <p align=\"center\">median value of owner-occupied homes in $\\$$1000s</p> |\n",
    "\n",
    "\n",
    "<img src=\"images/b1.png\" width=\"700\">\n",
    "\n",
    "Input and output layers of a MLP are determined by a considered problem. Specifically, the house price prediction problem predicts `medv` from a list of 13 features.\n",
    "\n",
    "<img src=\"images/b2.png\" width=\"700\">\n",
    "\n",
    "<img src=\"images/b3.png\" width=\"700\">\n",
    "\n",
    "#### Source \n",
    "\n",
    "Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81–102.\n",
    "\n",
    "Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.\n",
    "\n",
    "\n",
    "#### Chuẩn bị training data\n",
    "\n",
    "Dữ liệu được tách thành dữ liệu đặc trưng và ground truth (medv), được minh họa như hình sau:\n",
    "\n",
    "<img src=\"images/b4.png\" width=\"700\">\n",
    "\n",
    "Tensorflow cung cấp hàm `load_data()` được xây dựng sẵn để lấy dữ liệu giá nhà Boston. Hàm `load_data()` cung cấp training data để huấn luyện network và testing data để kiểm tra độ chính xác của network sau khi được huấn luyện. Ở bước này, để việc huấn luyện dễ dàng hơn, training data thường được chuẩn hóa (normalize) cho từng đặc trưng để đưa về giá trị trung bình (mean) bằng `0` và độ lệch chuẩn (standard deviation) bằng `1`.\n",
    "\n",
    "```python\n",
    "(train_features, train_labels), (test_features, test_labels) = tensorflow.keras.datasets.boston_housing.load_data()\n",
    "# (train_features, train_labels) is training data\n",
    "# (test_features, test_labels) is testing data\n",
    "\n",
    "# normalize\n",
    "train_mean = np.mean(train_features, axis=0)\n",
    "train_std = np.std(train_features, axis=0)\n",
    "train_features = (train_features - train_mean) / train_std\n",
    "```\n",
    "\n",
    "#### Model construction\n",
    "\n",
    "For instance, we want to construct a MLP that have a hidden layer, including 20 nodes. For the house price prediction problem, the input layer has 13 nodes and the output layer has 1 node.\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "                            Dense(20, activation=tf.nn.relu, input_shape=[13]), \n",
    "                            Dense(1)\n",
    "                        ])\n",
    "```\n",
    "\n",
    "# Ôn lại kiến thức numpy và matplotlib trước khi đọc source code\n",
    "\n",
    "## Introduction to numpy\n",
    "\n",
    "Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays.\n",
    "\n",
    "#### Arrays\n",
    "\n",
    "A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3])   # Create a rank 1 array\n",
    "print(type(a))            # Prints \"<class 'numpy.ndarray'>\"\n",
    "print(a.shape)            # Prints \"(3,)\"\n",
    "print(a[0], a[1], a[2])   # Prints \"1 2 3\"\n",
    "a[0] = 5                  # Change an element of the array\n",
    "print(a)                  # Prints \"[5, 2, 3]\"\n",
    "\n",
    "b = np.array([[1,2,3],[4,5,6]])    # Create a rank 2 array\n",
    "print(b.shape)                     # Prints \"(2, 3)\"\n",
    "print(b[0, 0], b[0, 1], b[1, 0])   # Prints \"1 2 4\"\n",
    "```\n",
    "\n",
    "Numpy also provides many functions to create arrays:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "a = np.zeros((2,2))   # Create an array of all zeros\n",
    "print(a)              # Prints \"[[ 0.  0.]\n",
    "                      #          [ 0.  0.]]\"\n",
    "\n",
    "b = np.ones((1,2))    # Create an array of all ones\n",
    "print(b)              # Prints \"[[ 1.  1.]]\"\n",
    "\n",
    "c = np.full((2,2), 7)  # Create a constant array\n",
    "print(c)               # Prints \"[[ 7.  7.]\n",
    "                       #          [ 7.  7.]]\"\n",
    "\n",
    "d = np.eye(2)         # Create a 2x2 identity matrix\n",
    "print(d)              # Prints \"[[ 1.  0.]\n",
    "                      #          [ 0.  1.]]\"\n",
    "\n",
    "e = np.random.random((2,2))  # Create an array filled with random values\n",
    "print(e)                     # Might print \"[[ 0.91940167  0.08143941]\n",
    "                             #               [ 0.68744134  0.87236687]]\"\n",
    "\n",
    "```\n",
    "\n",
    "#### Broadcasting\n",
    "\n",
    "Broadcasting is a powerful mechanism that allows numpy to work with arrays of different shapes when performing arithmetic operations. Frequently we have a smaller array and a larger array, and we want to use the smaller array multiple times to perform some operation on the larger array.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# We will add the vector v to each row of the matrix x,\n",
    "# storing the result in the matrix y\n",
    "x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
    "v = np.array([1, 0, 1])\n",
    "y = x + v  # Add v to each row of x using broadcasting\n",
    "print(y)  # Prints \"[[ 2  2  4]\n",
    "          #          [ 5  5  7]\n",
    "          #          [ 8  8 10]\n",
    "          #          [11 11 13]]\"\n",
    "\n",
    "```\n",
    "\n",
    "#### Mean function\n",
    "\n",
    "numpy.mean(arr, axis = None) : Compute the arithmetic mean (average) of the given data (array elements) along the specified axis. \n",
    "\n",
    "arr : input array.\n",
    "axis : axis along which we want to calculate the arithmetic mean. axis = 0 means along the column and axis = 1 means working along the row.\n",
    "\n",
    "```python\n",
    "import numpy as np \n",
    "    \n",
    "# 2D array  \n",
    "arr = [[4, 1, 1, 3, 4],   \n",
    "       [5, 6, 2, 8, 9],  \n",
    "       [3, 2, 5, 1, 4]]  \n",
    "    \n",
    "# mean along the axis = 0  \n",
    "print(\"\\nmean of arr, axis = 0 : \", np.mean(arr, axis = 0))\n",
    "# mean of arr, axis = 0 :  [4.  3.  5.  4.  6.]\n",
    "   \n",
    "# mean along the axis = 1  \n",
    "print(\"\\nmean of arr, axis = 1 : \", np.mean(arr, axis = 1))  \n",
    "#mean of arr, axis = 1 :  [3.8   6.  3.4]\n",
    "```\n",
    "\n",
    "#### Standard deviation function\n",
    "\n",
    "numpy.std(arr, axis = None) : Compute the standard deviation of the given data (array elements) along the specified axis(if any).\n",
    "\n",
    "Standard Deviation (SD) is measured as the spread of data distribution in the given data set.\n",
    "\n",
    "```python\n",
    "import numpy as np \n",
    "    \n",
    "# 2D array  \n",
    "arr = [[4, 1, 1, 3, 4],   \n",
    "       [5, 6, 2, 8, 9],  \n",
    "       [3, 2, 5, 1, 4]]  \n",
    "      \n",
    "# std along the axis = 0  \n",
    "print(\"\\nstd of arr, axis = 0 : \", np.std(arr, axis = 0))  \n",
    "#std of arr, axis = 0 :  [0.81  2.16  1.69  2.94  2.35]\n",
    "\n",
    "# std along the axis = 1  \n",
    "print(\"\\nstd of arr, axis = 1 : \", np.std(arr, axis = 1))\n",
    "#std of arr, axis = 1 :  [1.35   2.44   1.41]\n",
    "```\n",
    "\n",
    "#### Min and max functioins\n",
    "\n",
    "The min() and max() functions returns the minimum and maximum values of an ndarray object along the axis specified.\n",
    "\n",
    "```python\n",
    "import numpy as np \n",
    "    \n",
    "# 2D array  \n",
    "arr = [[4, 1, 1, 3, 4],   \n",
    "       [5, 6, 2, 8, 9],  \n",
    "       [3, 2, 5, 1, 4]]  \n",
    "      \n",
    "# std along the axis = 0  \n",
    "print(\"\\nstd of arr, axis = 0 : \", np.min(arr, axis = 0))  \n",
    "#std of arr, axis = 0 :  [3 1 1 1 4]\n",
    "\n",
    "# std along the axis = 1  \n",
    "print(\"\\nstd of arr, axis = 0 : \", np.max(arr, axis = 0))\n",
    "#std of arr, axis = 0 :  [5 6 5 8 9]\n",
    "```\n",
    "\n",
    "## Matplotlib\n",
    "\n",
    "The most important function in matplotlib is `plot`, which allows you to plot 2D data. Here is a simple example:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the x and y coordinates for points on a sine curve\n",
    "x = np.arange(0, 10, 0.1)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Plot the points using matplotlib\n",
    "plt.plot(x, y)\n",
    "plt.show()  # You must call plt.show() to make graphics appear.\n",
    "```\n",
    "\n",
    "Running this code produces the following plot:\n",
    "\n",
    "<img src=\"images/house1.png\" width=\"500\">\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the x and y coordinates for points on a sine curve\n",
    "y_sin = np.sin(x)\n",
    "y_cos = np.cos(x)\n",
    "\n",
    "# Plot the points using matplotlib\n",
    "plt.plot(x, y_sin)\n",
    "plt.plot(x, y_cos)\n",
    "plt.xlabel('x axis label')\n",
    "plt.ylabel('y axis label')\n",
    "plt.title('Sine and Cosine')\n",
    "plt.legend(['Sine', 'Cosine'])\n",
    "plt.plot(x, y)\n",
    "#plt.show()\n",
    "```\n",
    "\n",
    "Running this code produces the following plot:\n",
    "\n",
    "<img src=\"images/house2.png\" width=\"500\">\n",
    "\n",
    "\n",
    "# Source code\n",
    "\n",
    "#### Source code for training\n",
    "\n",
    "```python\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Commonly used modules\n",
    "import numpy as np\n",
    "\n",
    "# Images, plots, display, and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
    "\n",
    "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
    "train_mean = np.mean(train_features, axis=0)\n",
    "train_std = np.std(train_features, axis=0)\n",
    "train_features = (train_features - train_mean) / train_std\n",
    "\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]), Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "                  loss='mse',\n",
    "                  metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.fit(train_features, train_labels, epochs=500, verbose=1, validation_split = 0.1)\n",
    "model.save_weights('house_price_prediction.ckpt')\n",
    "```\n",
    "\n",
    "#### Source code for tesing 1\n",
    "\n",
    "```python\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Commonly used modules\n",
    "import numpy as np\n",
    "\n",
    "# Images, plots, display, and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        Dense(20, activation=tf.nn.relu, input_shape=[13]), Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "                  loss='mse',\n",
    "                  metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.load_weights('house_price_prediction.ckpt')\n",
    "\n",
    "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
    "train_mean = np.mean(train_features, axis=0)\n",
    "train_std = np.std(train_features, axis=0)\n",
    "\n",
    "test_features_norm = (test_features - train_mean) / train_std\n",
    "mse, _ = model.evaluate(test_features_norm, test_labels)\n",
    "rmse = np.sqrt(mse)\n",
    "print('Root Mean Square Error on test set: {}'.format(round(rmse, 3)))\n",
    "```\n",
    "\n",
    "#### Source code for tesing 2\n",
    "\n",
    "```python\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        Dense(20, activation=tf.nn.relu, input_shape=[13]), Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "                  loss='mse',\n",
    "                  metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.load_weights('house_price_prediction.ckpt')\n",
    "\n",
    "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
    "train_mean = np.mean(train_features, axis=0)\n",
    "train_std = np.std(train_features, axis=0)\n",
    "\n",
    "test_features_norm = (test_features - train_mean) / train_std\n",
    "\n",
    "\n",
    "test_predictions = model.predict(test_features_norm).flatten()\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Testing samples')\n",
    "plt.ylabel('Predicted and label values')\n",
    "plt.plot(np.arange(0,test_predictions.shape[0]),test_predictions, label='test_predictions')\n",
    "plt.plot(np.arange(0,test_predictions.shape[0]),test_labels, label='test_labels')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
